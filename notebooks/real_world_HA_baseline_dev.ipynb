{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sumolib\n",
    "import sys\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append('..')\n",
    "from util_folder.ml_utils.data_utils.data_loader_utils import RWIncidentDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** DATA SUMMARY: ***\n",
      "input_obs_full.shape=torch.Size([1024, 215, 1, 12, 3])\n",
      "input_time_full.shape=torch.Size([1024, 215, 1, 12, 4])\n",
      "target_full.shape=torch.Size([1024, 215, 4])\n",
      "incident_info_full.shape=torch.Size([1024, 4])\n",
      "network_info_full.shape=torch.Size([1024, 215, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load net and dataset info\n",
    "folder_path = f'../pems_data'\n",
    "incident_data_module = RWIncidentDataModule(folder_path = folder_path, transform='standardize', batch_size = 100, spatial_test=False)\n",
    "incident_data_module.setup()\n",
    "train_dataloader = incident_data_module.train_dataloader()\n",
    "train_dataset = train_dataloader.dataset\n",
    "val_dataloader = incident_data_module.val_dataloader()\n",
    "val_dataset = val_dataloader.dataset\n",
    "test_dataloader = incident_data_module.test_dataloader()\n",
    "test_dataset = test_dataloader.dataset\n",
    "\n",
    "#with open('../Simulation_scenarios/motorway/Results/incident_large/ind_to_edge.json') as f:\n",
    "    #ind_to_edge = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HA Baseline v1\n",
    "* The idea is to find the historical incident that have\n",
    "* * The same incident edge\n",
    "* * The same number of blocked lanes\n",
    "* * The time as close as possible with above constraints\n",
    "* I checked we have at least 3 candidates for each incident in the val set luckily\n",
    "* I'll return zero if no match exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.baselines.ha_models import HA_model_v1\n",
    "from torch import nn\n",
    "from torchmetrics import MeanAbsolutePercentageError, Accuracy\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "from torchmetrics.functional import precision_recall\n",
    "from util_folder.ml_utils.result_utils.metric_utils import masked_mape, MetricObj, generate_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_ie_idx = val_dataset.incident_info[1,0]\n",
    "cur_n_lanes = val_dataset.incident_info[1,1]\n",
    "hist_incident_info = train_dataset.incident_info[:]\n",
    "hist_net_info = train_dataset.network_info[:, :,0]\n",
    "\n",
    "edge_cost = hist_net_info[:,cur_ie_idx.int()].abs()\n",
    "lane_cost = (hist_incident_info[:,1] - cur_n_lanes).abs()\n",
    "combined_cost = edge_cost + lane_cost\n",
    "min_cost = combined_cost.min()\n",
    "combined_mask = combined_cost == min_cost\n",
    "matching_idxs = combined_mask.argwhere()\n",
    "\n",
    "metric_obj = MetricObj(bce_pos_weight=25.5191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([616, 215, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bce': tensor(1.1558),\n",
       " 'acc': tensor(0.9486),\n",
       " 'f1': tensor(0.1615),\n",
       " 'prcsn': tensor(0.1535),\n",
       " 'rcll': tensor(0.1703)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ha_model = HA_model_v1(train_dataset, emulated_dataset_size=616)\n",
    "ha_preds = ha_model.predict_dataset(cur_infos=test_dataset.incident_info,\n",
    "                         cur_input_data=None,\n",
    "                         cur_input_data_time=test_dataset.input_time_data)\n",
    "full_test_res = metric_obj.calc_metrics(y_hat=ha_preds,\n",
    "                                        y_true=test_dataset.target_data,\n",
    "                                        sigmoid_in_class=False\n",
    "                                        )\n",
    "full_test_res['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/manity/Quick_adap/quick_adap_to_incidents/notebooks/real_world_HA_baseline_dev.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdragon/home/manity/Quick_adap/quick_adap_to_incidents/notebooks/real_world_HA_baseline_dev.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ha_model \u001b[39m=\u001b[39m HA_model_v1(train_dataset, emulated_dataset_size\u001b[39m=\u001b[39;49m\u001b[39m5455\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon/home/manity/Quick_adap/quick_adap_to_incidents/notebooks/real_world_HA_baseline_dev.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m ha_preds \u001b[39m=\u001b[39m ha_model\u001b[39m.\u001b[39mpredict_dataset(cur_infos\u001b[39m=\u001b[39mtest_dataset\u001b[39m.\u001b[39mincident_info,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon/home/manity/Quick_adap/quick_adap_to_incidents/notebooks/real_world_HA_baseline_dev.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                          cur_input_data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon/home/manity/Quick_adap/quick_adap_to_incidents/notebooks/real_world_HA_baseline_dev.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                          cur_input_data_time\u001b[39m=\u001b[39mtest_dataset\u001b[39m.\u001b[39minput_time_data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon/home/manity/Quick_adap/quick_adap_to_incidents/notebooks/real_world_HA_baseline_dev.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m full_test_res \u001b[39m=\u001b[39m metric_obj\u001b[39m.\u001b[39mcalc_metrics(y_hat\u001b[39m=\u001b[39mha_preds,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon/home/manity/Quick_adap/quick_adap_to_incidents/notebooks/real_world_HA_baseline_dev.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                                         y_true\u001b[39m=\u001b[39mtest_dataset\u001b[39m.\u001b[39mtarget_data,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon/home/manity/Quick_adap/quick_adap_to_incidents/notebooks/real_world_HA_baseline_dev.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                                         sigmoid_in_class\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon/home/manity/Quick_adap/quick_adap_to_incidents/notebooks/real_world_HA_baseline_dev.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m                                         )\n",
      "File \u001b[0;32m~/Quick_adap/quick_adap_to_incidents/notebooks/../models/baselines/ha_models.py:53\u001b[0m, in \u001b[0;36mHA_model_v1.__init__\u001b[0;34m(self, train_dataset, emulated_dataset_size, random_seed)\u001b[0m\n\u001b[1;32m     51\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(random_seed)\n\u001b[1;32m     52\u001b[0m full_idxs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhist_incident_info))\n\u001b[0;32m---> 53\u001b[0m subset_idxs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(full_idxs, size\u001b[39m=\u001b[39;49memulated_dataset_size, replace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhist_incident_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhist_incident_info[subset_idxs]\n\u001b[1;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhist_ie_times \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhist_ie_times[subset_idxs]\n",
      "File \u001b[0;32mmtrand.pyx:965\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "ha_model = HA_model_v1(train_dataset, emulated_dataset_size=5455)\n",
    "ha_preds = ha_model.predict_dataset(cur_infos=test_dataset.incident_info,\n",
    "                         cur_input_data=None,\n",
    "                         cur_input_data_time=test_dataset.input_time_data)\n",
    "full_test_res = metric_obj.calc_metrics(y_hat=ha_preds,\n",
    "                                        y_true=test_dataset.target_data,\n",
    "                                        sigmoid_in_class=False\n",
    "                                        )\n",
    "full_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edge_idxs = torch.tensor([80, 81, 79, 53, 59, 64, 128, 15, 14, 100, 102, 101, 82, 83, 84, 85]) \n",
    "\n",
    "# generate masks for subbsets\n",
    "upstream_mask, multilane_mask, highway_mask, spreading_mask, affected_mask  = generate_masks(test_dataset)\n",
    "spatial_mask = sum(test_dataset.incident_info[...,0]==idx for idx in test_edge_idxs).bool().unsqueeze(-1).repeat(1, 147)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edge_idxs = torch.tensor([80, 81, 79, 53, 59, 64, 128, 15, 14, 100, 102, 101, 82, 83, 84, 85]) \n",
    "\n",
    "# generate masks for subbsets\n",
    "upstream_mask, multilane_mask, highway_mask, spreading_mask, affected_mask  = generate_masks(test_dataset)\n",
    "spatial_mask = sum(test_dataset.incident_info[...,0]==idx for idx in test_edge_idxs).bool().unsqueeze(-1).repeat(1, 147)\n",
    "\n",
    "spatial_test_res = metric_obj.calc_metrics(y_hat=ha_preds,\n",
    "                                           y_true=test_dataset.target_data,\n",
    "                                           mask=spatial_mask,\n",
    "                                           sigmoid_in_class=False\n",
    "                                           )\n",
    "\n",
    "spatial_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hop_dfs = {}\n",
    "for hop in range(11):\n",
    "    n_hop_mask = (test_dataset.network_info[:,:,0] == -hop)\n",
    "    n_hop_dfs[hop] = metric_obj.calc_metrics(y_hat=ha_preds,\n",
    "                                            y_true=test_dataset.target_data,\n",
    "                                            mask=n_hop_mask,\n",
    "                                            sigmoid_in_class=False\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\tf1\\tacc\\tprcsn\\trcll\\tst mae\\tend mae\\tsp mae')\n",
    "for hop in n_hop_dfs.keys():\n",
    "    temp_df = n_hop_dfs[hop]\n",
    "    f1 = temp_df['class']['f1']\n",
    "    acc = temp_df['class']['acc']\n",
    "    prcsn = temp_df['class']['prcsn']\n",
    "    rcll = temp_df['class']['rcll']\n",
    "    start_mae = temp_df['start']['mae'] \n",
    "    end_mae = temp_df['end']['mae'] \n",
    "    speed_mae = temp_df['speed']['mae'] \n",
    "    print(f'{hop}:\\t',\n",
    "          f'{f1:.02f}\\t',\n",
    "          f'{acc:.02f}\\t',\n",
    "          f'{prcsn:.02f}\\t',\n",
    "          f'{rcll:.02f}\\t',\n",
    "          f'{start_mae:.02f}\\t',\n",
    "          f'{end_mae:.02f}\\t',\n",
    "          f'{speed_mae:.02f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_len = train_dataset.incident_info.shape[0]\n",
    "subset_res_dicts = []\n",
    "subset_lens = []\n",
    "for i in np.arange(0.01,1.01,0.01):\n",
    "    subset_len = np.floor(hist_len * i).astype(int)\n",
    "\n",
    "    ha_model = HA_model_v1(train_dataset, emulated_dataset_size=subset_len)\n",
    "    ha_preds = ha_model.predict_dataset(cur_infos=test_dataset.incident_info,\n",
    "                            cur_input_data=None,\n",
    "                            cur_input_data_time=test_dataset.input_time_data)\n",
    "    subset_res = metric_obj.calc_metrics(y_hat=ha_preds,\n",
    "                            y_true=test_dataset.target_data,\n",
    "                            sigmoid_in_class=False\n",
    "                            )\n",
    "    \n",
    "    subset_lens.append(subset_len)\n",
    "    subset_res_dicts.append(subset_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = [res['class']['acc'] for res in subset_res_dicts]\n",
    "f1_scores = [res['class']['f1'] for res in subset_res_dicts]\n",
    "prec_scores = [res['class']['prcsn'] for res in subset_res_dicts]\n",
    "rec_scores = [res['class']['rcll'] for res in subset_res_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,4, figsize=(15,3))\n",
    "axs[0].plot(acc_scores)\n",
    "axs[0].set_title('Accuracy')\n",
    "axs[1].plot(rec_scores)\n",
    "axs[1].set_title('rcll')\n",
    "axs[2].plot(prec_scores)\n",
    "axs[2].set_title('prcsn')\n",
    "axs[3].plot(f1_scores)\n",
    "axs[3].set_title('f1')\n",
    "fig.tight_layout()\n",
    "[ax.set_ylim(0,1) for ax in axs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diferent dataset size test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_data_module = IncidentDataModule(folder_path = folder_path,\n",
    "                                          transform='standardize',\n",
    "                                          batch_size = 100,\n",
    "                                          spatial_test=False,\n",
    "                                          subset_size=100,                                          \n",
    "                                          )\n",
    "\n",
    "incident_data_module.setup()\n",
    "train_dataloader = incident_data_module.train_dataloader()\n",
    "train_dataset = train_dataloader.dataset\n",
    "val_dataloader = incident_data_module.val_dataloader()\n",
    "val_dataset = val_dataloader.dataset\n",
    "test_dataloader = incident_data_module.test_dataloader()\n",
    "test_dataset = test_dataloader.dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env_quick')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a98148271ae12b089ac623782357ec53fcf3ff4b348e20d09619b786609516cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
